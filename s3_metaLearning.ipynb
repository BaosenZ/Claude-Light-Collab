{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/data_full.csv')\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21588759",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['date'] != 20251028]\n",
    "print(df_train.shape)\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b785b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df['date'] == 20251028]\n",
    "print(df_test.shape)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save split data\n",
    "\n",
    "if True:\n",
    "    df_train.to_csv('data_split/data_train.csv', index=False)\n",
    "    df_test.to_csv('data_split/data_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd84749",
   "metadata": {},
   "source": [
    "## Meta-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tasks and scale data\n",
    "\n",
    "X_cols = ['R', 'G', 'B', 'time_of_day', 'weather_index', 'cloud_cover_percent']\n",
    "Y_cols = ['I_445', 'I_515', 'I_630', 'I_clear']\n",
    "\n",
    "grouped = df_train.groupby('day_of_year')\n",
    "\n",
    "tasks = []\n",
    "for day, df_day in grouped:\n",
    "    X_day = df_day[X_cols].to_numpy()\n",
    "    Y_day = df_day[Y_cols].to_numpy()\n",
    "    if len(X_day) < 10:\n",
    "        continue\n",
    "    tasks.append((day, X_day, Y_day))\n",
    "\n",
    "print(f\"Total days (tasks): {len(tasks)}\")\n",
    "\n",
    "X_all = df_train[X_cols].to_numpy()\n",
    "Y_all = df_train[Y_cols].to_numpy()\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "Y_scaler = StandardScaler()\n",
    "\n",
    "X_all_scaled = X_scaler.fit_transform(X_all)\n",
    "Y_all_scaled = Y_scaler.fit_transform(Y_all)\n",
    "\n",
    "tasks_scaled = []\n",
    "for day, _, _ in tasks:\n",
    "    mask = df_train['day_of_year'] == day\n",
    "    X_day = X_all_scaled[mask.to_numpy()]\n",
    "    Y_day = Y_all_scaled[mask.to_numpy()]\n",
    "    tasks_scaled.append((day, X_day, Y_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d21de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-learning config and loop\n",
    "\n",
    "input_dim = len(X_cols)\n",
    "output_dim = len(Y_cols)\n",
    "k_shot = 10\n",
    "meta_lr = 0.001\n",
    "inner_lr = 0.05\n",
    "meta_iters = 300\n",
    "np.random.seed(0)\n",
    "\n",
    "W_meta = np.random.randn(output_dim, input_dim) * 0.01\n",
    "b_meta = np.zeros(output_dim)\n",
    "\n",
    "for meta_iter in range(meta_iters):\n",
    "    dW_meta = np.zeros_like(W_meta)\n",
    "    db_meta = np.zeros_like(b_meta)\n",
    "    \n",
    "    for day, X_day, Y_day in tasks_scaled:\n",
    "        n = len(X_day)\n",
    "        if n < k_shot * 2:\n",
    "            continue  # skip days with too few samples\n",
    "        \n",
    "        # Randomly sample train/val split within this day\n",
    "        idx = np.random.permutation(n)\n",
    "        idx_train = idx[:k_shot]\n",
    "        idx_val = idx[k_shot:2*k_shot]\n",
    "        \n",
    "        X_train, Y_train = X_day[idx_train], Y_day[idx_train]\n",
    "        X_val, Y_val = X_day[idx_val], Y_day[idx_val]\n",
    "        \n",
    "        # Inner adaptation\n",
    "        Y_pred = X_train @ W_meta.T + b_meta\n",
    "        grad_W = (2 / k_shot) * (Y_pred - Y_train).T @ X_train\n",
    "        grad_b = (2 / k_shot) * np.sum(Y_pred - Y_train, axis=0)\n",
    "        W_task = W_meta - inner_lr * grad_W\n",
    "        b_task = b_meta - inner_lr * grad_b\n",
    "        \n",
    "        # Outer evaluation\n",
    "        Y_val_pred = X_val @ W_task.T + b_task\n",
    "        grad_W_meta = (2 / k_shot) * (Y_val_pred - Y_val).T @ X_val\n",
    "        grad_b_meta = (2 / k_shot) * np.sum(Y_val_pred - Y_val, axis=0)\n",
    "        \n",
    "        dW_meta += grad_W_meta\n",
    "        db_meta += grad_b_meta\n",
    "    \n",
    "    # Meta-update\n",
    "    W_meta -= meta_lr * dW_meta / len(tasks_scaled)\n",
    "    b_meta -= meta_lr * db_meta / len(tasks_scaled)\n",
    "    \n",
    "    if meta_iter % 50 == 0:\n",
    "        loss_meta = np.mean((Y_val_pred - Y_val)**2)\n",
    "        print(f\"Iter {meta_iter:03d} | Meta-loss: {loss_meta:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a68f86",
   "metadata": {},
   "source": [
    "## Predition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_shot = 10        \n",
    "inner_lr = 0.05\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Prepare columns ---\n",
    "X_cols = ['R','G','B','time_of_day','weather_index','cloud_cover_percent']\n",
    "Y_cols = ['I_445','I_515','I_630','I_clear']\n",
    "\n",
    "# --- Group test set by day ---\n",
    "test_days = sorted(df_test['day_of_year'].unique())\n",
    "\n",
    "results = []   # to store per-day metrics\n",
    "\n",
    "for day_target in test_days:\n",
    "    df_target = df_test[df_test['day_of_year'] == day_target]\n",
    "    if len(df_target) < 10:  # skip very small days\n",
    "        continue\n",
    "\n",
    "    # --- Extract and scale ---\n",
    "    X_target = df_target[X_cols].to_numpy()\n",
    "    Y_target = df_target[Y_cols].to_numpy()\n",
    "    X_target_scaled = X_scaler.transform(X_target)\n",
    "    Y_target_scaled = Y_scaler.transform(Y_target)\n",
    "\n",
    "    # --- Split into few-shot and eval sets ---\n",
    "    nT = len(X_target_scaled)\n",
    "    k = min(k_shot, nT // 2)\n",
    "    idx = np.random.choice(nT, size=k, replace=False)\n",
    "    mask = np.ones(nT, dtype=bool)\n",
    "    mask[idx] = False\n",
    "    X_adapt, Y_adapt = X_target_scaled[idx], Y_target_scaled[idx]\n",
    "    X_eval,  Y_eval  = X_target_scaled[mask], Y_target_scaled[mask]\n",
    "\n",
    "    # --- 1️⃣ BEFORE ADAPTATION ---\n",
    "    Y_pred_before_scaled = X_eval @ W_meta.T + b_meta\n",
    "    Y_pred_before = Y_scaler.inverse_transform(Y_pred_before_scaled)\n",
    "    Y_eval_orig   = Y_scaler.inverse_transform(Y_eval)\n",
    "\n",
    "    r2_before = [r2_score(Y_eval_orig[:, i], Y_pred_before[:, i]) for i in range(len(Y_cols))]\n",
    "    rmse_before = np.sqrt(np.mean((Y_eval_orig - Y_pred_before)**2, axis=0))\n",
    "\n",
    "    # --- 2️⃣ AFTER ADAPTATION ---\n",
    "    Y_pred_adapt = X_adapt @ W_meta.T + b_meta\n",
    "    grad_W = (2 / len(X_adapt)) * (Y_pred_adapt - Y_adapt).T @ X_adapt\n",
    "    grad_b = (2 / len(X_adapt)) * np.sum(Y_pred_adapt - Y_adapt, axis=0)\n",
    "    W_adapt = W_meta - inner_lr * grad_W\n",
    "    b_adapt = b_meta - inner_lr * grad_b\n",
    "\n",
    "    Y_pred_after_scaled = X_eval @ W_adapt.T + b_adapt\n",
    "    Y_pred_after = Y_scaler.inverse_transform(Y_pred_after_scaled)\n",
    "\n",
    "    r2_after = [r2_score(Y_eval_orig[:, i], Y_pred_after[:, i]) for i in range(len(Y_cols))]\n",
    "    rmse_after = np.sqrt(np.mean((Y_eval_orig - Y_pred_after)**2, axis=0))\n",
    "\n",
    "    results.append({\n",
    "        'day': day_target,\n",
    "        'r2_before': r2_before,\n",
    "        'r2_after': r2_after,\n",
    "        'rmse_before': rmse_before,\n",
    "        'rmse_after': rmse_after\n",
    "    })\n",
    "\n",
    "    print(f\"\\nDay {day_target} | samples={nT} | few-shot={k}\")\n",
    "    print(\"R² before:\", dict(zip(Y_cols, np.round(r2_before, 3))))\n",
    "    print(\"R² after :\", dict(zip(Y_cols, np.round(r2_after, 3))))\n",
    "    print(\"RMSE before:\", dict(zip(Y_cols, np.round(rmse_before, 3))))\n",
    "    print(\"RMSE after :\", dict(zip(Y_cols, np.round(rmse_after, 3))))\n",
    "\n",
    "\n",
    "# --- Aggregate average R² across all test days (per channel) ---\n",
    "r2_before_all = np.array([r['r2_before'] for r in results])  # shape (n_days, 4)\n",
    "r2_after_all  = np.array([r['r2_after']  for r in results])  # shape (n_days, 4)\n",
    "rmse_before_all = np.array([r['rmse_before'] for r in results])\n",
    "rmse_after_all  = np.array([r['rmse_after']  for r in results])\n",
    "\n",
    "avg_r2_before = np.mean(r2_before_all, axis=0)  # mean across days\n",
    "avg_r2_after  = np.mean(r2_after_all, axis=0)\n",
    "avg_rmse_before = np.mean(rmse_before_all, axis=0)\n",
    "avg_rmse_after  = np.mean(rmse_after_all, axis=0)\n",
    "\n",
    "print(\"\\n===========================================\")\n",
    "print(\"Average per-channel R² before adaptation:\")\n",
    "print(dict(zip(Y_cols, np.round(avg_r2_before, 3))))\n",
    "print(\"\\nAverage per-channel R² after adaptation:\")\n",
    "print(dict(zip(Y_cols, np.round(avg_r2_after, 3))))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Average per-channel RMSE before adaptation:\")\n",
    "print(dict(zip(Y_cols, np.round(avg_rmse_before, 3))))\n",
    "print(\"\\nAverage per-channel RMSE after adaptation:\")\n",
    "print(dict(zip(Y_cols, np.round(avg_rmse_after, 3))))\n",
    "print(\"===========================================\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
